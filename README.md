# Pyspark

Apache Spark is written in Scala programming language. To support Python with Spark, Apache Spark Community released a tool, PySpark. Using PySpark, you can work with RDDs in Python programming language also. It is because of a library called Py4j that they are able to achieve this.

PySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context. Majority of data scientists and analytics experts today use Python because of its rich library set. Integrating Python with Spark is a boon to them.

You can take the help from PySpark API documentation if you face any issue https://spark.apache.org/docs/latest/api/python/index.html

This Pyspark tutorials is purely practical with Examples. I am sharing the google colab notebook which includes examples for the below mentioned pyspark topics.

### Learning objectives - 
* Benefits of the Apache Spark ecosystem.
* Working with the DataFrame API.
* Working with columns and rows.
* Leveraging built-in Spark functions.
* Creating your own functions in Spark.
* Working with Resilient Distributed Datasets (RDDs).

----

Feel free to connect - https://www.linkedin.com/in/hsbhatia01/

___Happy Coding___
